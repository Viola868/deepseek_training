
------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14540866: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Thu Apr 24 21:47:34 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri Apr 25 11:31:44 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri Apr 25 11:31:44 2025
Terminated at Fri Apr 25 11:32:20 2025
Results reported at Fri Apr 25 11:32:20 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2.27 sec.
    Max Memory :                                 205 MB
    Average Memory :                             157.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              5
    Max Threads :                                6
    Run time :                                   39 sec.
    Turnaround time :                            49486 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-25 21:35:27,308] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 21:36:11,322] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-25 21:36:11,322] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-25 21:36:38,234] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-25 21:37:19,299] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-25 21:37:19,300] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-25 21:37:19,300] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-25 21:37:19,300] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-25 21:37:19,300] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-25 21:37:19,323] [INFO] [launch.py:256:main] process 102265 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-25 21:37:19,341] [INFO] [launch.py:256:main] process 102266 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-04-25 21:38:29,411] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 102265
[2025-04-25 21:38:29,412] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 102266
[2025-04-25 21:38:29,473] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14541778: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri Apr 25 12:21:51 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri Apr 25 21:34:46 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri Apr 25 21:34:46 2025
Terminated at Fri Apr 25 21:38:31 2025
Results reported at Fri Apr 25 21:38:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18.84 sec.
    Max Memory :                                 2802 MB
    Average Memory :                             1634.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   225 sec.
    Turnaround time :                            33400 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-27 14:02:13,544] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 14:02:40,260] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-27 14:02:40,261] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-27 14:02:54,120] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 14:03:21,459] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-27 14:03:21,459] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-27 14:03:21,459] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-27 14:03:21,459] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-27 14:03:21,459] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-27 14:03:21,484] [INFO] [launch.py:256:main] process 19870 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-27 14:03:21,506] [INFO] [launch.py:256:main] process 19871 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-04-27 14:04:03,551] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19870
[2025-04-27 14:04:03,613] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19871
[2025-04-27 14:04:03,613] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14547224: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun Apr 27 10:17:03 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun Apr 27 14:01:54 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun Apr 27 14:01:54 2025
Terminated at Sun Apr 27 14:04:05 2025
Results reported at Sun Apr 27 14:04:05 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   19.42 sec.
    Max Memory :                                 2704 MB
    Average Memory :                             1701.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   140 sec.
    Turnaround time :                            13622 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-27 18:53:16,910] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 18:53:46,108] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-27 18:53:46,108] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-27 18:54:01,246] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-27 18:54:29,297] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-27 18:54:29,297] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-27 18:54:29,297] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-27 18:54:29,297] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-27 18:54:29,297] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-27 18:54:29,317] [INFO] [launch.py:256:main] process 97249 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-27 18:54:29,335] [INFO] [launch.py:256:main] process 97250 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-04-27 18:55:40,427] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 97249
[2025-04-27 18:55:40,437] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 97250
[2025-04-27 18:55:40,438] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14548047: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun Apr 27 17:27:09 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun Apr 27 18:52:59 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun Apr 27 18:52:59 2025
Terminated at Sun Apr 27 18:55:42 2025
Results reported at Sun Apr 27 18:55:42 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.56 sec.
    Max Memory :                                 3019 MB
    Average Memory :                             2017.12 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   163 sec.
    Turnaround time :                            5313 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-30 16:24:32,392] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 16:25:46,104] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-30 16:25:46,149] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-30 16:26:35,286] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 16:27:42,837] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-30 16:27:42,837] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-30 16:27:42,837] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-30 16:27:42,837] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-30 16:27:42,837] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-30 16:27:42,903] [INFO] [launch.py:256:main] process 19968 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-30 16:27:42,926] [INFO] [launch.py:256:main] process 19969 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-04-30 16:30:00,070] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19968
[2025-04-30 16:30:00,080] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19969
[2025-04-30 16:30:00,080] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14555989: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 15:28:27 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 16:23:46 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Wed Apr 30 16:23:46 2025
Terminated at Wed Apr 30 16:30:06 2025
Results reported at Wed Apr 30 16:30:06 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.14 sec.
    Max Memory :                                 3004 MB
    Average Memory :                             1697.53 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   376 sec.
    Turnaround time :                            3699 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-30 18:56:05,282] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 18:57:04,143] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-30 18:57:04,293] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-30 18:57:40,942] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 18:58:37,028] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-30 18:58:37,046] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-30 18:58:37,046] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-30 18:58:37,046] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-30 18:58:37,046] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-30 18:58:37,121] [INFO] [launch.py:256:main] process 59230 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-30 18:58:37,140] [INFO] [launch.py:256:main] process 59231 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-04-30 19:00:45,270] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59230
[2025-04-30 19:00:45,477] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59231
[2025-04-30 19:00:45,478] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14556365: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 18:55:23 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 18:55:25 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Wed Apr 30 18:55:25 2025
Terminated at Wed Apr 30 19:00:47 2025
Results reported at Wed Apr 30 19:00:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.80 sec.
    Max Memory :                                 2936 MB
    Average Memory :                             1763.69 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   322 sec.
    Turnaround time :                            324 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-30 22:31:10,483] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 22:31:43,794] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-30 22:31:43,819] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-30 22:32:04,308] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 22:32:37,885] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-30 22:32:37,886] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-30 22:32:37,886] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-30 22:32:37,886] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-30 22:32:37,886] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-30 22:32:37,970] [INFO] [launch.py:256:main] process 108906 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-30 22:32:37,992] [INFO] [launch.py:256:main] process 108907 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-04-30 22:34:12,089] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 108906
[2025-04-30 22:34:12,100] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 108907
[2025-04-30 22:34:12,100] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14556582: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 22:30:45 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 22:30:46 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Wed Apr 30 22:30:46 2025
Terminated at Wed Apr 30 22:34:15 2025
Results reported at Wed Apr 30 22:34:15 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn/@Viola_P
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.24 sec.
    Max Memory :                                 3111 MB
    Average Memory :                             1972.22 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   208 sec.
    Turnaround time :                            210 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-04-30 23:35:50,287] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 23:36:44,588] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-04-30 23:36:44,623] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-04-30 23:37:14,229] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-04-30 23:38:04,595] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-04-30 23:38:04,595] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-04-30 23:38:04,595] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-04-30 23:38:04,595] [INFO] [launch.py:164:main] dist_world_size=2
[2025-04-30 23:38:04,595] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-04-30 23:38:04,724] [INFO] [launch.py:256:main] process 9926 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-04-30 23:38:04,745] [INFO] [launch.py:256:main] process 9927 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-04-30 23:40:08,874] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 9926
[2025-04-30 23:40:08,885] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 9927
[2025-04-30 23:40:08,897] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14556752: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 23:35:00 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 23:35:02 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Wed Apr 30 23:35:02 2025
Terminated at Wed Apr 30 23:40:10 2025
Results reported at Wed Apr 30 23:40:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.97 sec.
    Max Memory :                                 2939 MB
    Average Memory :                             1799.15 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   308 sec.
    Turnaround time :                            310 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-01 00:27:10,853] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 00:27:42,583] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-01 00:27:42,584] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-01 00:27:57,579] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 00:28:25,260] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-01 00:28:25,260] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-01 00:28:25,260] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-01 00:28:25,260] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-01 00:28:25,260] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-01 00:28:25,285] [INFO] [launch.py:256:main] process 17568 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-01 00:28:25,302] [INFO] [launch.py:256:main] process 17569 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-01 00:29:01,339] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17568
[2025-05-01 00:29:01,349] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17569
[2025-05-01 00:29:01,350] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14556803: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Wed Apr 30 23:57:19 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Thu May  1 00:26:49 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Thu May  1 00:26:49 2025
Terminated at Thu May  1 00:29:04 2025
Results reported at Thu May  1 00:29:04 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   18.34 sec.
    Max Memory :                                 2890 MB
    Average Memory :                             1821.86 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   133 sec.
    Turnaround time :                            1905 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-01 12:20:18,792] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 12:20:48,184] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-01 12:20:48,184] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-01 12:21:03,018] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 12:21:37,239] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-01 12:21:37,239] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-01 12:21:37,239] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-01 12:21:37,239] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-01 12:21:37,239] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-01 12:21:37,264] [INFO] [launch.py:256:main] process 77955 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-01 12:21:37,284] [INFO] [launch.py:256:main] process 77956 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-01 12:22:54,364] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77955
[2025-05-01 12:22:54,375] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77956
[2025-05-01 12:22:54,375] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14557546: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Thu May  1 12:20:05 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Thu May  1 12:20:05 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Thu May  1 12:20:05 2025
Terminated at Thu May  1 12:22:55 2025
Results reported at Thu May  1 12:22:55 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.04 sec.
    Max Memory :                                 3060 MB
    Average Memory :                             2029.12 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   172 sec.
    Turnaround time :                            170 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-01 22:32:14,816] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 22:34:21,672] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-01 22:34:21,673] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-01 22:35:02,394] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-01 22:36:20,838] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-01 22:36:20,839] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-01 22:36:20,839] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-01 22:36:20,839] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-01 22:36:20,839] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-01 22:36:20,871] [INFO] [launch.py:256:main] process 324 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-01 22:36:20,891] [INFO] [launch.py:256:main] process 325 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-01 22:38:31,023] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 324
[2025-05-01 22:38:31,033] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 325
[2025-05-01 22:38:31,033] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14557571: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Thu May  1 12:49:47 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Thu May  1 22:31:18 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Thu May  1 22:31:18 2025
Terminated at Thu May  1 22:38:32 2025
Results reported at Thu May  1 22:38:32 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.61 sec.
    Max Memory :                                 2948 MB
    Average Memory :                             1529.35 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   434 sec.
    Turnaround time :                            35325 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 13:05:49,948] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:06:19,632] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 13:06:19,632] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 13:06:33,682] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:07:01,195] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 13:07:01,196] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 13:07:01,196] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 13:07:01,196] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 13:07:01,196] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 13:07:01,221] [INFO] [launch.py:256:main] process 100674 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 13:07:01,244] [INFO] [launch.py:256:main] process 100675 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 13:08:16,322] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 100674
[2025-05-02 13:08:16,332] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 100675
[2025-05-02 13:08:16,333] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558883: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:05:25 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:05:27 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 13:05:27 2025
Terminated at Fri May  2 13:08:17 2025
Results reported at Fri May  2 13:08:17 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.41 sec.
    Max Memory :                                 2894 MB
    Average Memory :                             1877.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   170 sec.
    Turnaround time :                            172 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 13:20:13,203] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:20:40,933] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 13:20:40,934] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 13:20:54,711] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:21:21,357] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 13:21:21,357] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 13:21:21,357] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 13:21:21,357] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 13:21:21,357] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 13:21:21,379] [INFO] [launch.py:256:main] process 104446 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 13:21:21,393] [INFO] [launch.py:256:main] process 104447 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 13:22:20,454] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 104446
[2025-05-02 13:22:20,464] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 104447
[2025-05-02 13:22:20,464] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558889: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:19:56 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:19:57 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 13:19:57 2025
Terminated at Fri May  2 13:22:21 2025
Results reported at Fri May  2 13:22:21 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.39 sec.
    Max Memory :                                 2893 MB
    Average Memory :                             1855.14 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   17592186044415 MB
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   149 sec.
    Turnaround time :                            145 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 13:26:11,158] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:26:37,296] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 13:26:37,297] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 13:26:51,417] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:27:17,110] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 13:27:17,111] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 13:27:17,111] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 13:27:17,111] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 13:27:17,111] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 13:27:17,136] [INFO] [launch.py:256:main] process 106161 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 13:27:17,158] [INFO] [launch.py:256:main] process 106162 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 13:28:28,231] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 106161
[2025-05-02 13:28:28,241] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 106162
[2025-05-02 13:28:28,242] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558892: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:25:53 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:25:55 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 13:25:55 2025
Terminated at Fri May  2 13:28:29 2025
Results reported at Fri May  2 13:28:29 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.15 sec.
    Max Memory :                                 2791 MB
    Average Memory :                             988.50 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   154 sec.
    Turnaround time :                            156 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 13:36:35,259] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:37:14,450] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 13:37:14,450] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 13:37:49,769] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:38:56,577] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 13:38:56,577] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 13:38:56,577] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 13:38:56,577] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 13:38:56,577] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 13:38:56,599] [INFO] [launch.py:256:main] process 109240 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 13:38:56,614] [INFO] [launch.py:256:main] process 109241 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 13:40:19,720] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 109240
[2025-05-02 13:40:19,721] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 109241
[2025-05-02 13:40:19,731] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558907: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:36:05 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:36:07 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 13:36:07 2025
Terminated at Fri May  2 13:40:21 2025
Results reported at Fri May  2 13:40:21 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.07 sec.
    Max Memory :                                 2938 MB
    Average Memory :                             1788.64 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   254 sec.
    Turnaround time :                            256 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 13:45:06,084] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:45:38,659] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 13:45:38,660] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 13:45:55,793] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 13:46:28,534] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 13:46:28,534] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 13:46:28,534] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 13:46:28,534] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 13:46:28,534] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 13:46:28,559] [INFO] [launch.py:256:main] process 111344 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 13:46:28,580] [INFO] [launch.py:256:main] process 111345 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 13:48:08,684] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111344
[2025-05-02 13:48:08,694] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111345
[2025-05-02 13:48:08,695] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558913: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:44:50 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 13:44:51 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 13:44:51 2025
Terminated at Fri May  2 13:48:10 2025
Results reported at Fri May  2 13:48:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.05 sec.
    Max Memory :                                 3066 MB
    Average Memory :                             2102.56 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   199 sec.
    Turnaround time :                            200 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 14:16:06,630] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 14:16:37,812] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 14:16:37,818] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 14:16:53,781] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 14:17:23,874] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 14:17:23,874] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 14:17:23,874] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 14:17:23,874] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 14:17:23,874] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 14:17:23,899] [INFO] [launch.py:256:main] process 4890 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 14:17:23,921] [INFO] [launch.py:256:main] process 4891 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 14:18:59,019] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4890
[2025-05-02 14:18:59,029] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4891
[2025-05-02 14:18:59,030] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558941: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 14:15:47 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 14:15:48 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 14:15:48 2025
Terminated at Fri May  2 14:19:00 2025
Results reported at Fri May  2 14:19:00 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.17 sec.
    Max Memory :                                 2911 MB
    Average Memory :                             1998.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   192 sec.
    Turnaround time :                            193 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 14:51:44,598] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 14:52:33,913] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 14:52:34,000] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 14:52:54,307] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 14:53:34,795] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 14:53:34,796] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 14:53:34,796] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 14:53:34,796] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 14:53:34,796] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 14:53:34,814] [INFO] [launch.py:256:main] process 95060 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 14:53:34,827] [INFO] [launch.py:256:main] process 95061 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 14:55:20,937] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95060
[2025-05-02 14:55:20,938] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95061
[2025-05-02 14:55:20,947] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14558967: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 14:51:11 2025
Job was executed on host(s) <10*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 14:51:11 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 14:51:11 2025
Terminated at Fri May  2 14:55:22 2025
Results reported at Fri May  2 14:55:22 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.38 sec.
    Max Memory :                                 2943 MB
    Average Memory :                             1817.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   251 sec.
    Turnaround time :                            251 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 15:02:39,575] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:03:28,059] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 15:03:28,059] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 15:03:47,292] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:04:24,594] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 15:04:24,594] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 15:04:24,594] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 15:04:24,594] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 15:04:24,594] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 15:04:24,617] [INFO] [launch.py:256:main] process 16743 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 15:04:24,632] [INFO] [launch.py:256:main] process 16744 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 15:06:08,737] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 16743
[2025-05-02 15:06:08,737] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 16744
[2025-05-02 15:06:08,747] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558977: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:02:12 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:02:14 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 15:02:14 2025
Terminated at Fri May  2 15:06:10 2025
Results reported at Fri May  2 15:06:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.78 sec.
    Max Memory :                                 2918 MB
    Average Memory :                             1892.30 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   236 sec.
    Turnaround time :                            238 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 15:17:12,130] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:18:07,810] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 15:18:07,810] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 15:18:38,440] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:19:23,547] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 15:19:23,547] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 15:19:23,547] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 15:19:23,547] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 15:19:23,547] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 15:19:23,574] [INFO] [launch.py:256:main] process 20658 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 15:19:23,593] [INFO] [launch.py:256:main] process 20659 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 15:21:14,709] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 20658
[2025-05-02 15:21:14,709] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 20659
[2025-05-02 15:21:14,719] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14558995: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:16:47 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:16:48 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 15:16:48 2025
Terminated at Fri May  2 15:21:16 2025
Results reported at Fri May  2 15:21:16 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.03 sec.
    Max Memory :                                 2917 MB
    Average Memory :                             1770.45 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   268 sec.
    Turnaround time :                            269 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 15:38:45,318] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:39:22,955] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 15:39:22,956] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 15:39:49,310] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:40:30,788] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 15:40:30,788] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 15:40:30,788] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 15:40:30,788] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 15:40:30,788] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 15:40:30,810] [INFO] [launch.py:256:main] process 26074 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 15:40:30,825] [INFO] [launch.py:256:main] process 26075 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 15:41:50,905] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 26074
[2025-05-02 15:41:50,906] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 26075
[2025-05-02 15:41:50,915] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559018: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:38:22 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:38:23 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 15:38:23 2025
Terminated at Fri May  2 15:41:52 2025
Results reported at Fri May  2 15:41:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.39 sec.
    Max Memory :                                 3162 MB
    Average Memory :                             2000.22 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   209 sec.
    Turnaround time :                            210 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 15:46:44,383] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:47:25,303] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 15:47:25,304] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 15:47:50,084] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:48:28,286] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 15:48:28,286] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 15:48:28,286] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 15:48:28,286] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 15:48:28,286] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 15:48:28,308] [INFO] [launch.py:256:main] process 28274 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 15:48:28,323] [INFO] [launch.py:256:main] process 28275 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 15:49:45,402] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28274
[2025-05-02 15:49:45,412] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28275
[2025-05-02 15:49:45,413] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559030: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:46:21 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:46:22 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 15:46:22 2025
Terminated at Fri May  2 15:49:47 2025
Results reported at Fri May  2 15:49:47 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.72 sec.
    Max Memory :                                 2918 MB
    Average Memory :                             1841.89 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   204 sec.
    Turnaround time :                            206 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 15:59:06,545] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 15:59:46,799] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 15:59:46,799] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:00:03,562] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:00:37,887] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:00:37,887] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:00:37,887] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:00:37,887] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:00:37,887] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:00:37,909] [INFO] [launch.py:256:main] process 31487 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:00:37,924] [INFO] [launch.py:256:main] process 31488 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 16:00:38,925] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 31487
[2025-05-02 16:00:38,935] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 31488
[2025-05-02 16:00:38,935] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559046: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:58:38 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 15:58:38 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 15:58:38 2025
Terminated at Fri May  2 16:00:40 2025
Results reported at Fri May  2 16:00:40 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   12.17 sec.
    Max Memory :                                 1434 MB
    Average Memory :                             967.17 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              37
    Max Threads :                                73
    Run time :                                   122 sec.
    Turnaround time :                            122 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:03:59,116] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:04:40,922] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:04:40,922] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:04:59,984] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:05:38,399] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:05:38,399] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:05:38,399] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:05:38,399] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:05:38,399] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:05:38,423] [INFO] [launch.py:256:main] process 32960 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:05:38,441] [INFO] [launch.py:256:main] process 32961 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 16:06:49,514] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 32960
[2025-05-02 16:06:49,524] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 32961
[2025-05-02 16:06:49,525] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559050: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:03:35 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:03:37 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:03:37 2025
Terminated at Fri May  2 16:06:51 2025
Results reported at Fri May  2 16:06:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
#export SWANLAB_HOST=https://swanlab.cn
#export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.54 sec.
    Max Memory :                                 3072 MB
    Average Memory :                             1965.89 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                164
    Run time :                                   217 sec.
    Turnaround time :                            196 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:08:07,037] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:08:54,098] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:08:54,098] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:09:17,596] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:09:55,646] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:09:55,646] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:09:55,646] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:09:55,646] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:09:55,647] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:09:55,671] [INFO] [launch.py:256:main] process 34254 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:09:55,690] [INFO] [launch.py:256:main] process 34255 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 16:11:46,804] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 34254
[2025-05-02 16:11:46,814] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 34255
[2025-05-02 16:11:46,814] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559053: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:07:43 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:07:44 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:07:44 2025
Terminated at Fri May  2 16:11:48 2025
Results reported at Fri May  2 16:11:48 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.89 sec.
    Max Memory :                                 3028 MB
    Average Memory :                             1992.30 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   244 sec.
    Turnaround time :                            245 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:22:36,517] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:23:16,168] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:23:16,169] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:23:38,893] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:24:37,379] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:24:37,379] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:24:37,379] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:24:37,379] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:24:37,379] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:24:37,402] [INFO] [launch.py:256:main] process 38087 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:24:37,416] [INFO] [launch.py:256:main] process 38088 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 16:26:35,537] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 38087
[2025-05-02 16:26:35,547] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 38088
[2025-05-02 16:26:35,548] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559068: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:22:09 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:22:12 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:22:12 2025
Terminated at Fri May  2 16:26:37 2025
Results reported at Fri May  2 16:26:37 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.33 sec.
    Max Memory :                                 2978 MB
    Average Memory :                             1920.55 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   285 sec.
    Turnaround time :                            268 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:42:44,945] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:43:15,114] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:43:15,114] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:43:31,387] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:44:05,036] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:44:05,036] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:44:05,036] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:44:05,036] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:44:05,036] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:44:05,058] [INFO] [launch.py:256:main] process 43060 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:44:05,073] [INFO] [launch.py:256:main] process 43061 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 16:45:50,180] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43060
[2025-05-02 16:45:50,190] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43061
[2025-05-02 16:45:50,190] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559092: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:42:24 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:42:25 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:42:25 2025
Terminated at Fri May  2 16:45:51 2025
Results reported at Fri May  2 16:45:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.73 sec.
    Max Memory :                                 3043 MB
    Average Memory :                             2002.22 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   206 sec.
    Turnaround time :                            207 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:48:51,884] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:49:43,382] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:49:43,382] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:50:06,985] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:50:47,695] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 16:50:47,695] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 16:50:47,695] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 16:50:47,695] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 16:50:47,695] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 16:50:47,718] [INFO] [launch.py:256:main] process 44956 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 16:50:47,733] [INFO] [launch.py:256:main] process 44957 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 16:52:14,822] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 44956
[2025-05-02 16:52:14,832] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 44957
[2025-05-02 16:52:14,832] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559101: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:48:25 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:48:26 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:48:26 2025
Terminated at Fri May  2 16:52:16 2025
Results reported at Fri May  2 16:52:16 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.44 sec.
    Max Memory :                                 3038 MB
    Average Memory :                             1782.20 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   230 sec.
    Turnaround time :                            231 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 16:58:38,319] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 16:59:36,014] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 16:59:36,014] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 16:59:57,845] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:00:37,942] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 17:00:37,942] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 17:00:37,942] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 17:00:37,942] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 17:00:37,942] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 17:00:37,963] [INFO] [launch.py:256:main] process 47612 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 17:00:37,983] [INFO] [launch.py:256:main] process 47613 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 17:02:32,099] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47612
[2025-05-02 17:02:32,109] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47613
[2025-05-02 17:02:32,109] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559116: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:58:12 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 16:58:14 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 16:58:14 2025
Terminated at Fri May  2 17:02:33 2025
Results reported at Fri May  2 17:02:33 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.68 sec.
    Max Memory :                                 2970 MB
    Average Memory :                             1828.36 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   259 sec.
    Turnaround time :                            261 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 17:04:28,849] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:05:04,275] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 17:05:04,276] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 17:05:28,225] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:06:09,379] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 17:06:09,380] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 17:06:09,380] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 17:06:09,380] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 17:06:09,380] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 17:06:09,401] [INFO] [launch.py:256:main] process 49423 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 17:06:09,416] [INFO] [launch.py:256:main] process 49424 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: | Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: / Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: - Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[1m[34mswanlab[0m[0m: \ Waiting for the swanlab cloud response.[2025-05-02 17:07:45,512] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 49423
[2025-05-02 17:07:45,522] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 49424
[2025-05-02 17:07:45,523] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559120: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:04:11 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:04:12 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 17:04:12 2025
Terminated at Fri May  2 17:07:46 2025
Results reported at Fri May  2 17:07:46 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.56 sec.
    Max Memory :                                 2916 MB
    Average Memory :                             1545.90 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   214 sec.
    Turnaround time :                            215 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 17:18:50,705] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:19:30,668] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 17:19:30,668] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 17:19:47,139] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:20:20,132] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 17:20:20,132] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 17:20:20,132] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 17:20:20,133] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 17:20:20,133] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 17:20:20,154] [INFO] [launch.py:256:main] process 53207 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 17:20:20,169] [INFO] [launch.py:256:main] process 53208 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 17:21:39,250] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53207
[2025-05-02 17:21:39,260] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53208
[2025-05-02 17:21:39,261] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559130: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:18:26 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:18:27 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 17:18:27 2025
Terminated at Fri May  2 17:21:40 2025
Results reported at Fri May  2 17:21:40 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.37 sec.
    Max Memory :                                 3042 MB
    Average Memory :                             1913.44 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   193 sec.
    Turnaround time :                            194 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 17:25:10,678] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:25:46,175] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 17:25:46,175] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 17:26:09,637] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:27:16,881] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 17:27:16,881] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 17:27:16,881] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 17:27:16,881] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 17:27:16,881] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 17:27:16,906] [INFO] [launch.py:256:main] process 55149 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 17:27:16,920] [INFO] [launch.py:256:main] process 55150 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_172903-0a894550[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 17:29:04,030] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55149
[2025-05-02 17:29:04,080] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55150
[2025-05-02 17:29:04,080] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559134: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:24:50 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:24:52 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 17:24:52 2025
Terminated at Fri May  2 17:29:05 2025
Results reported at Fri May  2 17:29:05 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.60 sec.
    Max Memory :                                 3026 MB
    Average Memory :                             1951.09 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   253 sec.
    Turnaround time :                            255 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 17:41:43,186] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:42:27,674] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 17:42:27,674] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 17:42:55,520] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 17:43:31,011] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 17:43:31,011] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 17:43:31,011] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 17:43:31,011] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 17:43:31,011] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 17:43:31,039] [INFO] [launch.py:256:main] process 59375 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 17:43:31,062] [INFO] [launch.py:256:main] process 59376 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-3a0f98844899cfdc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-3a0f98844899cfdc/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
[2025-05-02 17:45:20,175] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59375
[2025-05-02 17:45:20,175] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59376
[2025-05-02 17:45:20,185] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559150: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:41:18 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 17:41:20 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 17:41:20 2025
Terminated at Fri May  2 17:45:21 2025
Results reported at Fri May  2 17:45:21 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.19 sec.
    Max Memory :                                 2946 MB
    Average Memory :                             1895.90 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   241 sec.
    Turnaround time :                            243 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 18:35:30,538] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:36:10,769] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 18:36:10,769] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 18:36:30,005] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:37:12,880] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 18:37:12,881] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 18:37:12,881] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 18:37:12,881] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 18:37:12,909] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 18:37:12,932] [INFO] [launch.py:256:main] process 72675 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 18:37:12,947] [INFO] [launch.py:256:main] process 72676 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-27199f17903bdabd/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-27199f17903bdabd/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...
Dataset json downloaded and prepared to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-27199f17903bdabd/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.
[2025-05-02 18:39:08,064] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 72675
[2025-05-02 18:39:08,074] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 72676
[2025-05-02 18:39:08,074] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559192: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:35:05 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:35:06 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 18:35:06 2025
Terminated at Fri May  2 18:39:09 2025
Results reported at Fri May  2 18:39:09 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.71 sec.
    Max Memory :                                 3060 MB
    Average Memory :                             1994.80 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   266 sec.
    Turnaround time :                            244 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 18:45:39,831] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:46:17,696] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 18:46:17,696] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 18:46:39,398] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:47:18,778] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 18:47:18,778] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 18:47:18,778] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 18:47:18,778] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 18:47:18,778] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 18:47:18,803] [INFO] [launch.py:256:main] process 75454 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 18:47:18,826] [INFO] [launch.py:256:main] process 75467 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 18:49:06,936] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75454
[2025-05-02 18:49:06,947] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75467
[2025-05-02 18:49:06,947] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559201: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:45:16 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:45:17 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 18:45:17 2025
Terminated at Fri May  2 18:49:08 2025
Results reported at Fri May  2 18:49:08 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.17 sec.
    Max Memory :                                 3077 MB
    Average Memory :                             2056.30 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   231 sec.
    Turnaround time :                            232 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 18:51:56,603] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:52:41,372] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 18:52:41,373] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 18:53:03,371] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:53:40,299] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 18:53:40,299] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 18:53:40,299] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 18:53:40,299] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 18:53:40,299] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 18:53:40,321] [INFO] [launch.py:256:main] process 77261 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 18:53:40,341] [INFO] [launch.py:256:main] process 77262 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-9ace079222601ae6/0.0.0...Downloading and preparing dataset json/default to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-9ace079222601ae6/0.0.0...

Dataset json downloaded and prepared to file:///share/home/zhangshanqi/.cache/huggingface/datasets/json/default-9ace079222601ae6/0.0.0. Subsequent calls will reuse this data.
[2025-05-02 18:55:09,432] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77261
[2025-05-02 18:55:09,442] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 77262
[2025-05-02 18:55:09,442] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559210: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:51:36 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:51:38 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 18:51:38 2025
Terminated at Fri May  2 18:55:11 2025
Results reported at Fri May  2 18:55:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.14 sec.
    Max Memory :                                 2955 MB
    Average Memory :                             1873.78 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   212 sec.
    Turnaround time :                            215 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 18:59:17,477] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 18:59:46,313] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 18:59:46,330] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 19:00:03,294] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:00:36,055] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 19:00:36,055] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 19:00:36,055] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 19:00:36,055] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 19:00:36,055] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 19:00:36,078] [INFO] [launch.py:256:main] process 79246 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 19:00:36,093] [INFO] [launch.py:256:main] process 79247 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 19:01:47,165] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79246
[2025-05-02 19:01:47,175] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79247
[2025-05-02 19:01:47,175] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559217: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:58:57 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 18:58:59 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 18:58:59 2025
Terminated at Fri May  2 19:01:48 2025
Results reported at Fri May  2 19:01:48 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.54 sec.
    Max Memory :                                 3116 MB
    Average Memory :                             2055.62 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   169 sec.
    Turnaround time :                            171 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 19:14:04,400] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:14:36,489] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 19:14:36,490] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 19:14:55,134] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:15:40,116] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 19:15:40,116] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 19:15:40,116] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 19:15:40,116] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 19:15:40,116] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 19:15:40,139] [INFO] [launch.py:256:main] process 83170 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 19:15:40,156] [INFO] [launch.py:256:main] process 83171 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 19:16:51,228] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 83170
[2025-05-02 19:16:51,238] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 83171
[2025-05-02 19:16:51,238] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559230: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:13:43 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:13:46 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 19:13:46 2025
Terminated at Fri May  2 19:16:52 2025
Results reported at Fri May  2 19:16:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.85 sec.
    Max Memory :                                 3108 MB
    Average Memory :                             1957.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   210 sec.
    Turnaround time :                            189 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 19:21:13,073] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:21:47,497] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 19:21:47,498] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 19:22:04,266] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:22:34,122] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 19:22:34,122] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 19:22:34,122] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 19:22:34,122] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 19:22:34,122] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 19:22:34,144] [INFO] [launch.py:256:main] process 85104 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 19:22:34,159] [INFO] [launch.py:256:main] process 85105 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_192727-138e946f[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_192727-3fce05a7[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 19:27:58,589] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:27:58,589] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 19:28:19,513] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 85104
[2025-05-02 19:28:19,523] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 85105
[2025-05-02 19:28:19,523] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559236: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:20:52 2025
Job was executed on host(s) <10*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:20:53 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 19:20:53 2025
Terminated at Fri May  2 19:28:21 2025
Results reported at Fri May  2 19:28:21 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=2:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 10

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   672.10 sec.
    Max Memory :                                 64787 MB
    Average Memory :                             18624.94 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                249
    Run time :                                   447 sec.
    Turnaround time :                            449 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 19:41:39,839] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:42:28,306] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 19:42:28,307] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 19:42:47,002] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:43:21,665] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 19:43:21,665] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 19:43:21,665] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 19:43:21,665] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 19:43:21,665] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 19:43:21,689] [INFO] [launch.py:256:main] process 90841 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 19:43:21,708] [INFO] [launch.py:256:main] process 90842 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 19:43:22,709] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90841
[2025-05-02 19:43:22,719] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90842
[2025-05-02 19:43:22,720] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559247: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:41:11 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:41:12 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 19:41:12 2025
Terminated at Fri May  2 19:43:24 2025
Results reported at Fri May  2 19:43:24 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   12.75 sec.
    Max Memory :                                 2305 MB
    Average Memory :                             1249.29 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              69
    Max Threads :                                138
    Run time :                                   132 sec.
    Turnaround time :                            133 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 19:47:20,306] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:47:49,229] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 19:47:49,229] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 19:48:05,640] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:48:46,489] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 19:48:46,490] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 19:48:46,490] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 19:48:46,490] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 19:48:46,490] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 19:48:46,517] [INFO] [launch.py:256:main] process 92371 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 19:48:46,554] [INFO] [launch.py:256:main] process 92372 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 19:55:08,020] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:55:08,020] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 19:55:30,607] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 19:55:30,607] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-02 19:55:30,607] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 19:55:35,969] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 92371
[2025-05-02 19:55:35,969] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 92372
[2025-05-02 19:55:35,979] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559251: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:47:01 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:47:02 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 19:47:02 2025
Terminated at Fri May  2 19:55:37 2025
Results reported at Fri May  2 19:55:37 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   648.04 sec.
    Max Memory :                                 64660 MB
    Average Memory :                             27074.95 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                242
    Run time :                                   515 sec.
    Turnaround time :                            516 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 19:59:57,968] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:00:34,351] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 20:00:34,352] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 20:00:56,852] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:01:28,027] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 20:01:28,027] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 20:01:28,027] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 20:01:28,027] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 20:01:28,027] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 20:01:28,050] [INFO] [launch.py:256:main] process 95975 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 20:01:28,063] [INFO] [launch.py:256:main] process 95976 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 20:06:46,872] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:06:46,873] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:07:06,115] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:07:06,116] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:07:06,117] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_200708-062ed7f9[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_200708-520ab11d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 20:07:25,427] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95975
[2025-05-02 20:07:25,427] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95976
[2025-05-02 20:07:25,437] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559260: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:59:35 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 19:59:37 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 19:59:37 2025
Terminated at Fri May  2 20:07:26 2025
Results reported at Fri May  2 20:07:26 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   613.76 sec.
    Max Memory :                                 67364 MB
    Average Memory :                             29191.78 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              137
    Max Threads :                                380
    Run time :                                   469 sec.
    Turnaround time :                            471 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 20:11:15,012] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:11:52,388] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 20:11:52,388] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 20:12:12,130] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:12:44,445] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 20:12:44,445] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 20:12:44,446] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 20:12:44,446] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 20:12:44,446] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 20:12:44,471] [INFO] [launch.py:256:main] process 99342 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 20:12:44,491] [INFO] [launch.py:256:main] process 99343 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 20:17:51,818] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:17:51,819] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:18:16,534] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:18:16,535] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:18:16,535] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_201818-58c808d7[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_201818-e9427876[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training[1m[34mswanlab[0m[0m:
 Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 20:18:34,849] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 99342
[2025-05-02 20:18:34,860] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 99343
[2025-05-02 20:18:34,860] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559271: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:10:56 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:10:58 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 20:10:58 2025
Terminated at Fri May  2 20:18:36 2025
Results reported at Fri May  2 20:18:36 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   647.56 sec.
    Max Memory :                                 67701 MB
    Average Memory :                             29336.94 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              135
    Max Threads :                                378
    Run time :                                   458 sec.
    Turnaround time :                            460 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 20:24:08,483] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:24:37,564] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 20:24:37,564] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 20:24:55,267] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:25:28,849] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 20:25:28,849] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 20:25:28,849] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 20:25:28,849] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 20:25:28,849] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 20:25:28,872] [INFO] [launch.py:256:main] process 103104 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 20:25:28,895] [INFO] [launch.py:256:main] process 103105 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 20:31:33,958] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:31:33,958] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:31:51,143] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:31:51,143] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-02 20:31:51,144] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_203153-10755f8c[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_203153-67100fc8[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 20:32:12,305] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 103104
[2025-05-02 20:32:12,315] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 103105
[2025-05-02 20:32:12,315] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559283: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:23:52 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:23:53 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 20:23:53 2025
Terminated at Fri May  2 20:32:13 2025
Results reported at Fri May  2 20:32:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   641.62 sec.
    Max Memory :                                 67924 MB
    Average Memory :                             27450.26 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              137
    Max Threads :                                380
    Run time :                                   500 sec.
    Turnaround time :                            501 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 20:44:51,177] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:45:25,508] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 20:45:25,509] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 20:45:45,161] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:46:17,269] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 20:46:17,269] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 20:46:17,270] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 20:46:17,270] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 20:46:17,270] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 20:46:17,290] [INFO] [launch.py:256:main] process 63175 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 20:46:17,304] [INFO] [launch.py:256:main] process 63177 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 20:52:06,961] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:52:06,961] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 20:52:29,031] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:52:29,031] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 20:52:29,031] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_205231-1cd46693[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_205231-88840330[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 20:52:54,711] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 63175
[2025-05-02 20:52:54,711] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 63177
[2025-05-02 20:52:54,721] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14559313: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:44:25 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 20:44:26 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 20:44:26 2025
Terminated at Fri May  2 20:52:56 2025
Results reported at Fri May  2 20:52:56 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   684.45 sec.
    Max Memory :                                 67920 MB
    Average Memory :                             27732.37 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              135
    Max Threads :                                378
    Run time :                                   539 sec.
    Turnaround time :                            511 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 21:27:19,442] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:27:51,295] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 21:27:51,295] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 21:28:05,544] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:28:32,485] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 21:28:32,485] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 21:28:32,485] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 21:28:32,485] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 21:28:32,485] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 21:28:32,509] [INFO] [launch.py:256:main] process 4983 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 21:28:32,524] [INFO] [launch.py:256:main] process 4984 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 21:28:33,526] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4983
[2025-05-02 21:28:33,536] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4984
[2025-05-02 21:28:33,536] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559345: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:26:53 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:26:55 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 21:26:55 2025
Terminated at Fri May  2 21:28:35 2025
Results reported at Fri May  2 21:28:35 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   12.65 sec.
    Max Memory :                                 2254 MB
    Average Memory :                             1387.83 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              69
    Max Threads :                                138
    Run time :                                   100 sec.
    Turnaround time :                            102 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 21:31:33,221] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:32:09,982] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 21:32:09,983] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 21:32:29,372] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:33:03,998] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 21:33:03,998] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 21:33:03,998] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 21:33:03,998] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 21:33:03,999] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 21:33:04,024] [INFO] [launch.py:256:main] process 6383 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 21:33:04,046] [INFO] [launch.py:256:main] process 6384 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 21:38:49,397] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 6383
[2025-05-02 21:38:49,485] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 6384
[2025-05-02 21:38:49,485] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559364: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:31:13 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:31:14 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 21:31:14 2025
Terminated at Fri May  2 21:38:51 2025
Results reported at Fri May  2 21:38:51 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   649.90 sec.
    Max Memory :                                 64747 MB
    Average Memory :                             27100.06 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                238
    Run time :                                   457 sec.
    Turnaround time :                            458 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 21:43:32,133] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:44:06,388] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 21:44:06,389] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 21:44:25,450] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:45:02,523] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 21:45:02,523] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 21:45:02,523] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 21:45:02,523] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 21:45:02,523] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 21:45:02,548] [INFO] [launch.py:256:main] process 9742 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 21:45:02,569] [INFO] [launch.py:256:main] process 9743 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 21:50:21,388] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:50:21,388] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 21:50:39,919] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 21:50:39,919] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 21:50:39,919] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_215042-85d38692[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_215042-add7a092[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 21:51:23,961] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 9742
[2025-05-02 21:51:23,971] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 9743
[2025-05-02 21:51:23,972] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559478: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:43:06 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:43:07 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 21:43:07 2025
Terminated at Fri May  2 21:51:25 2025
Results reported at Fri May  2 21:51:25 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   849.25 sec.
    Max Memory :                                 96213 MB
    Average Memory :                             34227.74 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              137
    Max Threads :                                397
    Run time :                                   498 sec.
    Turnaround time :                            499 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 21:59:24,764] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:00:18,464] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 22:00:18,465] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 22:00:46,465] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:01:30,231] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 22:01:30,231] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 22:01:30,231] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 22:01:30,231] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 22:01:30,231] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 22:01:30,258] [INFO] [launch.py:256:main] process 14455 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 22:01:30,304] [INFO] [launch.py:256:main] process 14456 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 22:06:38,597] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:06:38,597] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:06:59,206] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 22:06:59,206] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-02 22:06:59,207] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 22:07:04,656] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 14455
[2025-05-02 22:07:04,667] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 14456
[2025-05-02 22:07:04,667] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559516: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:58:55 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 21:58:56 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 21:58:56 2025
Terminated at Fri May  2 22:07:06 2025
Results reported at Fri May  2 22:07:06 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   646.65 sec.
    Max Memory :                                 67774 MB
    Average Memory :                             26139.21 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              81
    Max Threads :                                260
    Run time :                                   490 sec.
    Turnaround time :                            491 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-02 22:12:34,385] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:13:05,724] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-02 22:13:05,724] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-02 22:13:27,135] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:14:06,812] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-02 22:14:06,812] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-02 22:14:06,812] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-02 22:14:06,812] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-02 22:14:06,812] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-02 22:14:06,834] [INFO] [launch.py:256:main] process 18108 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-02 22:14:06,856] [INFO] [launch.py:256:main] process 18109 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-02 22:19:55,324] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:19:55,324] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-02 22:20:18,143] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 22:20:18,144] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-02 22:20:18,144] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_222020-e7873818[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250502_222020-8190cb6f[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1/3] /share/apps/gcc/gcc10.2/bin/g++ -MMD -MF cpu_adam.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/TH -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/THC -isystem /share/home/zhangshanqi/zyh/cuda12.1/include -isystem /share/home/zhangshanqi/pytorch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/share/home/zhangshanqi/zyh/cuda12.1/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam.cpp -o cpu_adam.o 
[2/3] /share/apps/gcc/gcc10.2/bin/g++ -MMD -MF cpu_adam_impl.o.d -DTORCH_EXTENSION_NAME=cpu_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\"_gcc\" -DPYBIND11_STDLIB=\"_libstdcpp\" -DPYBIND11_BUILD_ABI=\"_cxxabi1011\" -I/share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/deepspeed/ops/csrc/includes -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/TH -isystem /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/include/THC -isystem /share/home/zhangshanqi/zyh/cuda12.1/include -isystem /share/home/zhangshanqi/pytorch/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -O3 -std=c++17 -g -Wno-reorder -L/share/home/zhangshanqi/zyh/cuda12.1/lib64 -lcudart -lcublas -g -march=native -fopenmp -D__AVX512__ -D__ENABLE_CUDA__ -c /share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/deepspeed/ops/csrc/adam/cpu_adam_impl.cpp -o cpu_adam_impl.o 
[3/3] /share/apps/gcc/gcc10.2/bin/g++ cpu_adam.o cpu_adam_impl.o -shared -lcurand -L/share/home/zhangshanqi/zyh/cuda12.1/lib64 -L/share/home/zhangshanqi/pytorch/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/share/home/zhangshanqi/zyh/cuda12.1/lib64 -lcudart -o cpu_adam.so
Time to load cpu_adam op: 104.81885981559753 seconds
Time to load cpu_adam op: 104.879967212677 seconds
Parameter Offload: Total persistent parameters: 266240 in 65 params
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-02 22:23:30,465] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 18108
[2025-05-02 22:23:30,465] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 18109
[2025-05-02 22:23:32,673] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14559602: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 22:12:03 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Fri May  2 22:12:04 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Fri May  2 22:12:04 2025
Terminated at Fri May  2 22:23:34 2025
Results reported at Fri May  2 22:23:34 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1573.29 sec.
    Max Memory :                                 115263 MB
    Average Memory :                             42276.96 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   17592186044415 MB
    Max Processes :                              140
    Max Threads :                                403
    Run time :                                   690 sec.
    Turnaround time :                            691 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 01:53:19,919] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 01:53:53,093] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=3,0,1,2 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 01:53:53,093] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 01:54:07,853] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 01:54:35,558] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 01:54:35,558] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 01:54:35,558] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 01:54:35,558] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 01:54:35,558] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 01:54:35,578] [INFO] [launch.py:256:main] process 19235 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 01:54:35,591] [INFO] [launch.py:256:main] process 19236 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 01:55:42,660] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19235
[2025-05-03 01:55:42,670] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 19236
[2025-05-03 01:55:42,670] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14559627: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Fri May  2 22:32:35 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 01:52:53 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 01:52:53 2025
Terminated at Sat May  3 01:55:44 2025
Results reported at Sat May  3 01:55:44 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   21.13 sec.
    Max Memory :                                 2998 MB
    Average Memory :                             1813.38 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   173 sec.
    Turnaround time :                            12189 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 12:50:28,348] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 12:51:04,966] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 12:51:04,966] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 12:51:26,224] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 12:52:13,371] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 12:52:13,371] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 12:52:13,371] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 12:52:13,371] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 12:52:13,371] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 12:52:13,392] [INFO] [launch.py:256:main] process 53858 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 12:52:13,405] [INFO] [launch.py:256:main] process 53859 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 12:57:29,840] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 12:57:29,840] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 12:57:47,904] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 12:57:47,904] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 12:57:47,904] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_125750-c863abac[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_125750-c00108ae[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 12:58:05,792] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53858
[2025-05-03 12:58:05,802] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53859
[2025-05-03 12:58:05,803] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560421: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 12:49:59 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 12:50:01 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 12:50:01 2025
Terminated at Sat May  3 12:58:07 2025
Results reported at Sat May  3 12:58:07 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   983.98 sec.
    Max Memory :                                 3511 MB
    Average Memory :                             2691.11 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                244
    Run time :                                   495 sec.
    Turnaround time :                            488 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 14:09:27,712] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 14:09:55,689] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 14:09:55,689] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 14:10:13,520] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 14:10:47,573] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 14:10:47,574] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 14:10:47,574] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 14:10:47,574] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 14:10:47,574] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 14:10:47,596] [INFO] [launch.py:256:main] process 72293 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 14:10:47,658] [INFO] [launch.py:256:main] process 72294 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 14:11:56,730] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 72293
[2025-05-03 14:11:56,740] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 72294
[2025-05-03 14:11:56,741] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560473: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 14:09:10 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 14:09:12 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 14:09:12 2025
Terminated at Sat May  3 14:11:58 2025
Results reported at Sat May  3 14:11:58 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.96 sec.
    Max Memory :                                 2891 MB
    Average Memory :                             1919.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   196 sec.
    Turnaround time :                            168 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:11:11,935] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:11:45,461] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:11:45,480] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:12:04,402] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:12:30,646] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:12:30,646] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:12:30,646] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:12:30,646] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:12:30,646] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:12:30,665] [INFO] [launch.py:256:main] process 86463 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:12:30,683] [INFO] [launch.py:256:main] process 86464 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:13:41,758] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 86463
[2025-05-03 15:13:41,768] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 86464
[2025-05-03 15:13:41,768] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560516: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:10:52 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:10:53 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:10:53 2025
Terminated at Sat May  3 15:13:43 2025
Results reported at Sat May  3 15:13:43 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.94 sec.
    Max Memory :                                 2901 MB
    Average Memory :                             1918.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   170 sec.
    Turnaround time :                            171 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:23:38,030] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:24:30,008] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:24:30,009] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:24:49,956] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:25:30,838] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:25:30,838] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:25:30,839] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:25:30,839] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:25:30,839] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:25:30,865] [INFO] [launch.py:256:main] process 89651 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:25:30,889] [INFO] [launch.py:256:main] process 89652 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:26:42,964] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 89651
[2025-05-03 15:26:42,974] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 89652
[2025-05-03 15:26:42,974] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560527: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:23:15 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:23:17 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:23:17 2025
Terminated at Sat May  3 15:26:44 2025
Results reported at Sat May  3 15:26:44 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.60 sec.
    Max Memory :                                 3090 MB
    Average Memory :                             1946.56 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   207 sec.
    Turnaround time :                            209 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:37:59,118] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:38:36,175] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:38:36,176] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:38:51,043] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:39:23,173] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:39:23,173] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:39:23,173] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:39:23,173] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:39:23,173] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:39:23,198] [INFO] [launch.py:256:main] process 93025 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:39:23,222] [INFO] [launch.py:256:main] process 93026 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:40:12,290] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 93025
[2025-05-03 15:40:12,290] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 93026
[2025-05-03 15:40:12,300] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560540: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:37:39 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:37:40 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:37:40 2025
Terminated at Sat May  3 15:40:13 2025
Results reported at Sat May  3 15:40:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.83 sec.
    Max Memory :                                 2942 MB
    Average Memory :                             1864.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                140
    Run time :                                   175 sec.
    Turnaround time :                            154 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:42:14,813] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:43:03,630] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:43:03,630] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:43:19,689] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:44:09,044] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:44:09,044] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:44:09,044] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:44:09,044] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:44:09,044] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:44:09,066] [INFO] [launch.py:256:main] process 94343 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:44:09,081] [INFO] [launch.py:256:main] process 94344 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:45:05,143] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 94343
[2025-05-03 15:45:05,153] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 94344
[2025-05-03 15:45:05,153] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560544: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:41:54 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:41:55 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:41:55 2025
Terminated at Sat May  3 15:45:07 2025
Results reported at Sat May  3 15:45:07 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.42 sec.
    Max Memory :                                 2999 MB
    Average Memory :                             1887.11 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                140
    Run time :                                   192 sec.
    Turnaround time :                            193 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:48:25,452] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:48:59,129] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:48:59,129] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:49:13,661] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:49:44,898] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:49:44,898] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:49:44,898] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:49:44,898] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:49:44,899] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:49:44,923] [INFO] [launch.py:256:main] process 95801 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:49:44,944] [INFO] [launch.py:256:main] process 95802 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:50:49,009] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95801
[2025-05-03 15:50:49,009] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 95802
[2025-05-03 15:50:49,071] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560549: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:48:09 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:48:10 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:48:10 2025
Terminated at Sat May  3 15:50:50 2025
Results reported at Sat May  3 15:50:50 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   20.85 sec.
    Max Memory :                                 3060 MB
    Average Memory :                             2050.62 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                140
    Run time :                                   160 sec.
    Turnaround time :                            161 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 15:53:42,290] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:54:26,130] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 15:54:26,130] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 15:54:46,816] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 15:55:37,509] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 15:55:37,509] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 15:55:37,510] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 15:55:37,510] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 15:55:37,510] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 15:55:37,537] [INFO] [launch.py:256:main] process 97364 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 15:55:37,553] [INFO] [launch.py:256:main] process 97365 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 15:56:48,629] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 97364
[2025-05-03 15:56:48,651] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 97365
[2025-05-03 15:56:48,652] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560561: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:53:24 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 15:53:25 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 15:53:25 2025
Terminated at Sat May  3 15:56:50 2025
Results reported at Sat May  3 15:56:50 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   24.27 sec.
    Max Memory :                                 3224 MB
    Average Memory :                             2081.89 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                140
    Run time :                                   205 sec.
    Turnaround time :                            206 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 16:11:20,223] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:11:58,348] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 16:11:58,349] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 16:12:17,157] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:12:56,232] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 16:12:56,232] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 16:12:56,232] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 16:12:56,232] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 16:12:56,232] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 16:12:56,255] [INFO] [launch.py:256:main] process 101510 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 16:12:56,276] [INFO] [launch.py:256:main] process 101511 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 16:14:14,356] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 101510
[2025-05-03 16:14:14,356] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 101511
[2025-05-03 16:14:14,366] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560575: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:11:00 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:11:01 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 16:11:01 2025
Terminated at Sat May  3 16:14:16 2025
Results reported at Sat May  3 16:14:16 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   23.70 sec.
    Max Memory :                                 3157 MB
    Average Memory :                             2063.44 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              71
    Max Threads :                                158
    Run time :                                   195 sec.
    Turnaround time :                            196 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 16:28:19,764] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:29:10,133] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 16:29:10,134] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 16:29:26,948] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:30:08,286] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 16:30:08,286] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 16:30:08,286] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 16:30:08,286] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 16:30:08,286] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 16:30:08,308] [INFO] [launch.py:256:main] process 105682 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 16:30:08,323] [INFO] [launch.py:256:main] process 105683 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

[2025-05-03 16:36:08,179] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:36:08,328] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:36:18,809] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 16:36:18,810] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 16:36:18,810] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_163620-2ba45093[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_163620-c2e9a35d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 16:36:36,731] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 105682
[2025-05-03 16:36:36,741] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 105683
[2025-05-03 16:36:36,741] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560581: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:27:54 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:27:56 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 16:27:56 2025
Terminated at Sat May  3 16:36:38 2025
Results reported at Sat May  3 16:36:38 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   614.37 sec.
    Max Memory :                                 6672 MB
    Average Memory :                             3540.30 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              137
    Max Threads :                                383
    Run time :                                   522 sec.
    Turnaround time :                            524 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 16:44:27,370] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:45:08,827] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 16:45:08,828] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 16:45:29,571] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:46:13,474] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2025-05-03 16:46:13,474] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-05-03 16:46:13,474] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-05-03 16:46:13,474] [INFO] [launch.py:164:main] dist_world_size=2
[2025-05-03 16:46:13,474] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2025-05-03 16:46:13,501] [INFO] [launch.py:256:main] process 109907 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 16:46:13,517] [INFO] [launch.py:256:main] process 109908 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 16:51:43,242] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:51:43,243] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 16:52:02,085] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 16:52:02,085] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 16:52:02,085] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_165204-014c53d9[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_165204-3eea002b[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 2.9369728565216064 secondsTime to load cpu_adam op: 2.937023401260376 seconds

Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 16:52:41,933] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 109907
[2025-05-03 16:52:43,144] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 109908
[2025-05-03 16:52:43,144] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560606: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:44:08 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 16:44:09 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 16:44:09 2025
Terminated at Sat May  3 16:52:44 2025
Results reported at Sat May  3 16:52:44 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

~/pytorch/bin/deepspeed --num_gpus=2 training_swanlab.py



------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   781.00 sec.
    Max Memory :                                 6209 MB
    Average Memory :                             2633.10 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              137
    Max Threads :                                382
    Run time :                                   515 sec.
    Turnaround time :                            516 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 17:20:52,170] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 17:21:29,078] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 17:21:29,079] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 17:21:43,571] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 17:22:20,742] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 17:22:20,742] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 17:22:20,742] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 17:22:20,743] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 17:22:20,743] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 17:22:20,766] [INFO] [launch.py:256:main] process 4441 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 17:22:20,786] [INFO] [launch.py:256:main] process 4442 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 17:22:20,809] [INFO] [launch.py:256:main] process 4443 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 17:22:20,834] [INFO] [launch.py:256:main] process 4444 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 17:30:51,373] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4441
[2025-05-03 17:30:51,395] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4442
[2025-05-03 17:30:51,404] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4443
[2025-05-03 17:30:51,412] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 4444
[2025-05-03 17:30:51,412] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560621: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:20:34 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:20:35 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 17:20:35 2025
Terminated at Sat May  3 17:30:53 2025
Results reported at Sat May  3 17:30:53 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1008.06 sec.
    Max Memory :                                 5350 MB
    Average Memory :                             4023.52 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   618 sec.
    Turnaround time :                            619 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 17:38:26,495] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 17:39:03,107] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 17:39:03,108] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 17:39:20,762] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 17:39:50,980] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 17:39:50,980] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 17:39:50,980] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 17:39:50,980] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 17:39:50,980] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 17:39:51,002] [INFO] [launch.py:256:main] process 8774 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 17:39:51,017] [INFO] [launch.py:256:main] process 8775 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 17:39:51,030] [INFO] [launch.py:256:main] process 8776 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 17:39:51,041] [INFO] [launch.py:256:main] process 8777 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 17:47:36,524] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 8774
[2025-05-03 17:47:36,588] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 8775
[2025-05-03 17:47:36,729] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 8776
[2025-05-03 17:47:36,729] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 8777
[2025-05-03 17:47:36,737] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560636: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:38:00 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:38:01 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 17:38:01 2025
Terminated at Sat May  3 17:47:38 2025
Results reported at Sat May  3 17:47:38 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1012.38 sec.
    Max Memory :                                 5163 MB
    Average Memory :                             4122.29 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   577 sec.
    Turnaround time :                            578 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 17:58:59,180] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 17:59:54,380] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 17:59:54,381] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 18:00:11,139] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:00:58,280] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 18:00:58,280] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 18:00:58,280] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 18:00:58,280] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 18:00:58,280] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 18:00:58,305] [INFO] [launch.py:256:main] process 13815 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 18:00:58,325] [INFO] [launch.py:256:main] process 13816 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 18:00:58,339] [INFO] [launch.py:256:main] process 13817 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 18:00:58,353] [INFO] [launch.py:256:main] process 13818 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 18:02:08,426] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 13815
[2025-05-03 18:02:08,436] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 13816
[2025-05-03 18:02:08,445] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 13817
[2025-05-03 18:02:08,445] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 13818
[2025-05-03 18:02:08,452] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560768: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:58:31 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 17:58:32 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 17:58:32 2025
Terminated at Sat May  3 18:02:10 2025
Results reported at Sat May  3 18:02:10 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   40.08 sec.
    Max Memory :                                 3645 MB
    Average Memory :                             1247.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                142
    Run time :                                   218 sec.
    Turnaround time :                            219 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 18:12:37,618] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:13:12,428] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 18:13:12,429] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 18:13:28,476] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:14:05,480] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 18:14:05,481] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 18:14:05,481] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 18:14:05,481] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 18:14:05,481] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 18:14:05,503] [INFO] [launch.py:256:main] process 17054 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 18:14:05,523] [INFO] [launch.py:256:main] process 17055 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 18:14:05,545] [INFO] [launch.py:256:main] process 17056 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 18:14:05,565] [INFO] [launch.py:256:main] process 17057 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 18:22:46,130] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17054
[2025-05-03 18:22:46,147] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17055
[2025-05-03 18:22:46,156] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17056
[2025-05-03 18:22:46,156] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 17057
[2025-05-03 18:22:46,163] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560770: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:12:22 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:12:23 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 18:12:23 2025
Terminated at Sat May  3 18:22:48 2025
Results reported at Sat May  3 18:22:48 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1527.52 sec.
    Max Memory :                                 65418 MB
    Average Memory :                             29451.44 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                342
    Run time :                                   634 sec.
    Turnaround time :                            626 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 18:31:05,626] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:31:55,122] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 18:31:55,122] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 18:32:19,569] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:33:07,831] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 18:33:07,831] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 18:33:07,831] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 18:33:07,839] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 18:33:07,839] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 18:33:07,861] [INFO] [launch.py:256:main] process 21725 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 18:33:07,876] [INFO] [launch.py:256:main] process 21726 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 18:33:07,889] [INFO] [launch.py:256:main] process 21727 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 18:33:07,900] [INFO] [launch.py:256:main] process 21728 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

[2025-05-03 18:42:19,996] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:42:19,996] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:42:20,056] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:42:20,260] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:42:32,250] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 18:42:32,250] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 18:42:32,251] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 18:42:32,251] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 18:42:32,251] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_184234-b0de3002[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_184234-377c7375[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_184234-7a15afbd[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_184234-c0b1f47d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.4100840091705322 secondsTime to load cpu_adam op: 3.4101197719573975 secondsTime to load cpu_adam op: 3.410130500793457 secondsTime to load cpu_adam op: 3.4101977348327637 seconds



Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: [1m[34mswanlab[0m[0m:Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally 
ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 18:43:29,646] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 21725
[2025-05-03 18:43:29,656] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 21726
[2025-05-03 18:43:29,671] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 21727
[2025-05-03 18:43:29,671] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 21728
[2025-05-03 18:43:29,679] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560779: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:30:37 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:30:38 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 18:30:38 2025
Terminated at Sat May  3 18:43:31 2025
Results reported at Sat May  3 18:43:31 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2371.49 sec.
    Max Memory :                                 136589 MB
    Average Memory :                             43202.71 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              205
    Max Threads :                                667
    Run time :                                   773 sec.
    Turnaround time :                            774 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 18:51:59,954] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:52:35,174] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 18:52:35,174] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 18:52:50,179] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 18:53:37,988] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 18:53:37,988] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 18:53:37,989] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 18:53:37,989] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 18:53:37,989] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 18:53:38,014] [INFO] [launch.py:256:main] process 27594 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 18:53:38,030] [INFO] [launch.py:256:main] process 27595 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 18:53:38,048] [INFO] [launch.py:256:main] process 27596 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 18:53:38,069] [INFO] [launch.py:256:main] process 27597 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 19:01:58,582] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 27594
[2025-05-03 19:01:58,620] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 27595
[2025-05-03 19:01:58,628] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 27596
[2025-05-03 19:01:58,628] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 27597
[2025-05-03 19:01:58,636] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560791: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:51:41 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 18:51:43 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 18:51:43 2025
Terminated at Sat May  3 19:02:00 2025
Results reported at Sat May  3 19:02:00 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1013.99 sec.
    Max Memory :                                 6119 MB
    Average Memory :                             4077.26 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   617 sec.
    Turnaround time :                            619 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 19:32:53,574] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:33:26,529] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 19:33:26,529] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 19:33:39,950] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:34:10,856] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 19:34:10,857] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 19:34:10,857] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 19:34:10,857] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 19:34:10,883] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 19:34:10,905] [INFO] [launch.py:256:main] process 39358 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 19:34:10,920] [INFO] [launch.py:256:main] process 39359 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 19:34:10,938] [INFO] [launch.py:256:main] process 39360 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 19:34:10,959] [INFO] [launch.py:256:main] process 39361 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 19:42:47,635] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:42:47,635] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:42:47,635] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:42:47,636] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 19:42:58,149] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 19:42:58,149] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 19:42:58,149] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 19:42:58,149] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 19:42:58,149] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_194300-00bb1602[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_194300-c6daf0f3[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_194300-85254578[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_194300-f47eb01d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:  ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locallyğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally

[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 19:43:16,571] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39358
[2025-05-03 19:43:16,582] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39359
[2025-05-03 19:43:16,591] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39360
[2025-05-03 19:43:16,591] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39361
[2025-05-03 19:43:16,598] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560806: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 19:32:31 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 19:32:32 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 19:32:32 2025
Terminated at Sat May  3 19:43:18 2025
Results reported at Sat May  3 19:43:18 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1299.02 sec.
    Max Memory :                                 11070 MB
    Average Memory :                             5153.08 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              205
    Max Threads :                                635
    Run time :                                   646 sec.
    Turnaround time :                            647 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 20:04:07,198] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:04:53,957] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 20:04:53,957] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 20:05:14,631] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:05:51,821] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 20:05:51,822] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 20:05:51,822] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 20:05:51,822] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 20:05:51,822] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 20:05:51,843] [INFO] [launch.py:256:main] process 47737 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 20:05:51,858] [INFO] [launch.py:256:main] process 47738 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 20:05:51,873] [INFO] [launch.py:256:main] process 47739 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 20:05:51,889] [INFO] [launch.py:256:main] process 47740 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 20:13:56,411] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47737
[2025-05-03 20:13:56,474] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47738
[2025-05-03 20:13:56,483] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47739
[2025-05-03 20:13:56,490] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 47740
[2025-05-03 20:13:56,490] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560819: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 20:03:45 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 20:03:46 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 20:03:46 2025
Terminated at Sat May  3 20:13:58 2025
Results reported at Sat May  3 20:13:58 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1011.25 sec.
    Max Memory :                                 8410 MB
    Average Memory :                             4023.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   612 sec.
    Turnaround time :                            613 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 20:29:16,184] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:29:47,985] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 20:29:47,985] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 20:30:01,201] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:30:28,898] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 20:30:28,898] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 20:30:28,898] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 20:30:28,898] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 20:30:28,898] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 20:30:28,922] [INFO] [launch.py:256:main] process 53600 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 20:30:28,938] [INFO] [launch.py:256:main] process 53601 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 20:30:28,977] [INFO] [launch.py:256:main] process 53602 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 20:30:28,990] [INFO] [launch.py:256:main] process 53603 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 20:38:50,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:38:50,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:38:50,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:38:50,092] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 20:39:02,056] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 20:39:02,056] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 20:39:02,056] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 20:39:02,056] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 20:39:02,056] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_203903-793baf54[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_203903-d305c69f[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_203903-73aa8fa5[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_203903-50458efc[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.2535107135772705 seconds
Time to load cpu_adam op: 3.282820463180542 seconds
Time to load cpu_adam op: 3.293531894683838 seconds
Time to load cpu_adam op: 3.3002705574035645 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training[1m[34mswanlab[0m[0m:
  Error happened while trainingError happened while training

 Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:
[1m[34mswanlab[0m[0m:  ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locallyğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally 

ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 20:39:53,619] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53600
[2025-05-03 20:39:54,201] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53601
[2025-05-03 20:39:54,211] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53602
[2025-05-03 20:39:54,212] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 53603
[2025-05-03 20:39:54,220] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560846: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 20:28:56 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 20:28:57 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 20:28:57 2025
Terminated at Sat May  3 20:39:56 2025
Results reported at Sat May  3 20:39:56 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2370.97 sec.
    Max Memory :                                 141793 MB
    Average Memory :                             47136.59 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              203
    Max Threads :                                626
    Run time :                                   658 sec.
    Turnaround time :                            660 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 21:01:52,430] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:02:25,080] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 21:02:25,081] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 21:02:38,847] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:03:08,663] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 21:03:08,664] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 21:03:08,664] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 21:03:08,664] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 21:03:08,664] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 21:03:08,685] [INFO] [launch.py:256:main] process 62232 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 21:03:08,705] [INFO] [launch.py:256:main] process 62233 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 21:03:08,726] [INFO] [launch.py:256:main] process 62234 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 21:03:08,739] [INFO] [launch.py:256:main] process 62239 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 21:10:55,219] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 62232
[2025-05-03 21:10:55,239] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 62233
[2025-05-03 21:10:55,247] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 62234
[2025-05-03 21:10:55,247] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 62239
[2025-05-03 21:10:55,255] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560865: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:01:33 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:01:35 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 21:01:35 2025
Terminated at Sat May  3 21:10:56 2025
Results reported at Sat May  3 21:10:56 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1007.38 sec.
    Max Memory :                                 8302 MB
    Average Memory :                             4234.67 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   561 sec.
    Turnaround time :                            563 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 21:29:49,038] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:30:22,229] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 21:30:22,229] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 21:30:36,347] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:31:08,050] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 21:31:08,051] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 21:31:08,051] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 21:31:08,051] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 21:31:08,051] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 21:31:08,078] [INFO] [launch.py:256:main] process 68863 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 21:31:08,099] [INFO] [launch.py:256:main] process 68864 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 21:31:08,130] [INFO] [launch.py:256:main] process 68865 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 21:31:08,145] [INFO] [launch.py:256:main] process 68866 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 21:40:37,977] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:40:37,977] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:40:37,977] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:40:37,977] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:40:48,782] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 21:40:48,782] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 21:40:48,782] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 21:40:48,782] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 21:40:48,783] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_214050-22f13ec5[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_214050-4381126a[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_214050-ecb85b38[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_214050-c7b2e97a[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.1924006938934326 secondsTime to load cpu_adam op: 3.192416191101074 secondsTime to load cpu_adam op: 3.192403793334961 secondsTime to load cpu_adam op: 3.1924402713775635 seconds



Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:   ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locallyğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locallyğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m:

 ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 21:41:43,854] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 68863
[2025-05-03 21:41:43,864] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 68864
[2025-05-03 21:41:43,898] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 68865
[2025-05-03 21:41:43,906] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 68866
[2025-05-03 21:41:43,906] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560880: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:29:28 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:29:29 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 21:29:29 2025
Terminated at Sat May  3 21:41:45 2025
Results reported at Sat May  3 21:41:45 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2367.98 sec.
    Max Memory :                                 137237 MB
    Average Memory :                             45075.21 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              201
    Max Threads :                                667
    Run time :                                   736 sec.
    Turnaround time :                            737 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 21:57:06,196] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:57:47,235] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 21:57:47,235] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 21:58:00,139] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 21:58:32,312] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 21:58:32,312] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 21:58:32,312] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 21:58:32,312] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 21:58:32,312] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 21:58:32,337] [INFO] [launch.py:256:main] process 76214 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 21:58:32,354] [INFO] [launch.py:256:main] process 76215 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 21:58:32,368] [INFO] [launch.py:256:main] process 76216 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 21:58:32,384] [INFO] [launch.py:256:main] process 76217 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 21:58:33,385] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 76214
[2025-05-03 21:58:33,395] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 76215
[2025-05-03 21:58:33,404] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 76216
[2025-05-03 21:58:33,404] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 76217
[2025-05-03 21:58:33,412] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560894: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:56:47 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 21:56:49 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 21:56:49 2025
Terminated at Sat May  3 21:58:35 2025
Results reported at Sat May  3 21:58:35 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.23 sec.
    Max Memory :                                 2576 MB
    Average Memory :                             1619.00 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              69
    Max Threads :                                138
    Run time :                                   106 sec.
    Turnaround time :                            108 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 22:05:54,504] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:06:35,228] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 22:06:35,229] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 22:06:47,463] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:07:19,610] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 22:07:19,610] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 22:07:19,610] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 22:07:19,610] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 22:07:19,610] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 22:07:19,634] [INFO] [launch.py:256:main] process 78463 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 22:07:19,657] [INFO] [launch.py:256:main] process 78464 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 22:07:19,676] [INFO] [launch.py:256:main] process 78465 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 22:07:19,714] [INFO] [launch.py:256:main] process 78466 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-03 22:07:36,732] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 78463
[2025-05-03 22:07:36,742] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 78464
[2025-05-03 22:07:36,751] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 78465
[2025-05-03 22:07:36,751] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 78466
[2025-05-03 22:07:36,759] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560898: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:05:34 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:05:35 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 22:05:35 2025
Terminated at Sat May  3 22:07:38 2025
Results reported at Sat May  3 22:07:38 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   19.91 sec.
    Max Memory :                                 2411 MB
    Average Memory :                             1505.33 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              69
    Max Threads :                                138
    Run time :                                   123 sec.
    Turnaround time :                            124 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 22:09:45,416] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:10:17,124] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-03 22:10:17,124] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-03 22:10:31,742] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:11:02,580] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-03 22:11:02,580] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-03 22:11:02,580] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-03 22:11:02,580] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-03 22:11:02,580] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-03 22:11:02,604] [INFO] [launch.py:256:main] process 79514 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-03 22:11:02,621] [INFO] [launch.py:256:main] process 79515 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-03 22:11:02,635] [INFO] [launch.py:256:main] process 79516 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-03 22:11:02,647] [INFO] [launch.py:256:main] process 79517 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 22:20:15,168] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:20:15,168] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:20:15,168] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:20:15,168] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 22:20:26,334] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 22:20:26,335] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 22:20:26,335] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 22:20:26,335] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 22:20:26,335] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_222027-02345bc3[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_222027-4b0fe74f[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_222027-a7a4b8ae[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_222027-7e369534[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.193931818008423 seconds
Time to load cpu_adam op: 3.2159574031829834 seconds
Time to load cpu_adam op: 3.2235617637634277 seconds
Time to load cpu_adam op: 3.2278285026550293 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 [1m[34mswanlab[0m[0m:Error happened while training
 Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally 
ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-03 22:21:25,347] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79514
[2025-05-03 22:21:25,357] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79515
[2025-05-03 22:21:25,366] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79516
[2025-05-03 22:21:25,366] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 79517
[2025-05-03 22:21:25,373] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560901: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:09:37 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:09:39 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 22:09:39 2025
Terminated at Sat May  3 22:21:27 2025
Results reported at Sat May  3 22:21:27 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2384.81 sec.
    Max Memory :                                 157942 MB
    Average Memory :                             48289.20 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              205
    Max Threads :                                622
    Run time :                                   708 sec.
    Turnaround time :                            710 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 22:50:38,071] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560949: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:50:12 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 22:50:13 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 22:50:13 2025
Terminated at Sat May  3 22:51:05 2025
Results reported at Sat May  3 22:51:05 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“


accelerate launch \
  --num_processes 4 \
  --deepspeed_config_file /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/ds_config.json \ 
  training_swanlab.py


------------------------------------------------------------

Exited with exit code 127.

Resource usage summary:

    CPU time :                                   6.98 sec.
    Max Memory :                                 308 MB
    Average Memory :                             266.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                6
    Run time :                                   52 sec.
    Turnaround time :                            53 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 23:03:08,808] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 23:06:23,096] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:06:23,096] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:06:23,097] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:06:23,124] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:06:33,600] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:06:33,600] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:06:33,600] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 23:06:33,601] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:06:33,601] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_230635-83cb6eb4[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_230635-0355ac27[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_230635-69525adf[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_230635-771d784a[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.3080809116363525 secondsTime to load cpu_adam op: 3.3080806732177734 secondsTime to load cpu_adam op: 3.308103561401367 seconds


Time to load cpu_adam op: 3.308336019515991 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally[1m[34mswanlab[0m[0m:
 ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560959: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 23:02:45 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 23:02:47 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 23:02:47 2025
Terminated at Sat May  3 23:07:15 2025
Results reported at Sat May  3 23:07:15 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

accelerate launch \
  --num_processes 4 \
  --deepspeed_config_file /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/ds_config.json \
  /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/training_swanlab.py


 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   476.87 sec.
    Max Memory :                                 131989 MB
    Average Memory :                             58191.54 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              176
    Max Threads :                                371
    Run time :                                   268 sec.
    Turnaround time :                            270 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-03 23:16:58,471] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-03 23:20:21,094] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:20:21,094] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:20:21,094] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:20:21,094] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-03 23:20:31,505] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:20:31,505] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-03 23:20:31,505] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:20:31,505] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-03 23:20:31,506] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_232032-047a5e2f[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_232032-d09140e4[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_232032-b42c6f3d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250503_232032-6a663d44[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.228292465209961 secondsTime to load cpu_adam op: 3.228342294692993 secondsTime to load cpu_adam op: 3.2283430099487305 seconds


Time to load cpu_adam op: 3.2681992053985596 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:  Error happened while trainingError happened while training

[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: [1m[34mswanlab[0m[0m:ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
 ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu05>
Subject: Job 14560970: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 23:16:33 2025
Job was executed on host(s) <20*gpu05>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sat May  3 23:16:35 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sat May  3 23:16:35 2025
Terminated at Sat May  3 23:21:13 2025
Results reported at Sat May  3 23:21:13 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

accelerate launch \
  --num_processes 4 \
  --deepspeed_config_file /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/ds_config.json \
  /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/training_swanlab.py


 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   479.72 sec.
    Max Memory :                                 124266 MB
    Average Memory :                             28689.75 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   17592186044415 MB
    Max Processes :                              172
    Max Threads :                                368
    Run time :                                   277 sec.
    Turnaround time :                            280 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 13:27:32,015] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561031: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sat May  3 23:56:04 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 13:27:00 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 13:27:00 2025
Terminated at Sun May  4 13:29:29 2025
Results reported at Sun May  4 13:29:29 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„åº“

accelerate launch \
  --num_processes 4 \
  --deepspeed_config_file /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/ds_config.json \
  /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/training_swanlab.py


 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   29.48 sec.
    Max Memory :                                 2373 MB
    Average Memory :                             1611.71 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              40
    Max Threads :                                76
    Run time :                                   149 sec.
    Turnaround time :                            48805 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 14:14:32,227] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:15:10,220] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 14:15:10,220] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 14:15:31,144] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:16:12,097] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 14:16:12,097] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 14:16:12,097] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 14:16:12,097] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 14:16:12,097] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 14:16:12,124] [INFO] [launch.py:256:main] process 37358 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 14:16:12,148] [INFO] [launch.py:256:main] process 37359 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 14:16:12,175] [INFO] [launch.py:256:main] process 37360 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 14:16:12,201] [INFO] [launch.py:256:main] process 37361 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-04 14:16:13,203] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 37358
[2025-05-04 14:16:13,214] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 37359
[2025-05-04 14:16:13,222] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 37360
[2025-05-04 14:16:13,222] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 37361
[2025-05-04 14:16:13,230] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561476: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:14:10 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:14:12 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 14:14:12 2025
Terminated at Sun May  4 14:16:14 2025
Results reported at Sun May  4 14:16:14 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.45 sec.
    Max Memory :                                 1479 MB
    Average Memory :                             1034.17 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              37
    Max Threads :                                73
    Run time :                                   122 sec.
    Turnaround time :                            124 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 14:21:25,942] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:22:06,149] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 14:22:06,149] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 14:22:24,401] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:23:09,894] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 14:23:09,894] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 14:23:09,894] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 14:23:09,894] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 14:23:09,894] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 14:23:09,920] [INFO] [launch.py:256:main] process 39285 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 14:23:09,937] [INFO] [launch.py:256:main] process 39286 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 14:23:09,976] [INFO] [launch.py:256:main] process 39287 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 14:23:09,990] [INFO] [launch.py:256:main] process 39288 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-04 14:32:28,576] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39285
[2025-05-04 14:32:28,760] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39286
[2025-05-04 14:32:28,768] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39287
[2025-05-04 14:32:28,768] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 39288
[2025-05-04 14:32:28,776] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561477: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:21:04 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:21:06 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 14:21:06 2025
Terminated at Sun May  4 14:32:30 2025
Results reported at Sun May  4 14:32:30 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1560.42 sec.
    Max Memory :                                 65297 MB
    Average Memory :                             28510.88 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                342
    Run time :                                   684 sec.
    Turnaround time :                            686 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 14:36:47,108] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:37:26,544] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 14:37:26,544] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 14:37:41,216] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:38:23,073] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 14:38:23,073] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 14:38:23,073] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 14:38:23,073] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 14:38:23,073] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 14:38:23,102] [INFO] [launch.py:256:main] process 43260 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 14:38:23,118] [INFO] [launch.py:256:main] process 43261 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 14:38:23,131] [INFO] [launch.py:256:main] process 43262 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 14:38:23,144] [INFO] [launch.py:256:main] process 43263 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

[2025-05-04 14:47:48,342] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:47:48,342] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:47:48,342] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:47:48,342] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 14:48:00,032] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 14:48:00,032] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 14:48:00,033] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 14:48:00,033] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 14:48:00,034] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_144801-ad4e54c4[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_144801-0b9c5b3b[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_144801-755cfa85[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_144801-09ee0820[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.1116156578063965 secondsTime to load cpu_adam op: 3.1116089820861816 secondsTime to load cpu_adam op: 3.1115827560424805 seconds


Time to load cpu_adam op: 3.1742000579833984 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally 
ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 14:48:59,847] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43260
[2025-05-04 14:48:59,857] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43261
[2025-05-04 14:48:59,866] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43262
[2025-05-04 14:48:59,866] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 43263
[2025-05-04 14:48:59,873] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561485: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:36:16 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 14:36:18 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 14:36:18 2025
Terminated at Sun May  4 14:49:03 2025
Results reported at Sun May  4 14:49:03 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2417.93 sec.
    Max Memory :                                 154219 MB
    Average Memory :                             45638.71 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              201
    Max Threads :                                667
    Run time :                                   763 sec.
    Turnaround time :                            767 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 15:01:06,588] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:01:48,681] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 15:01:48,681] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 15:02:03,947] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:02:58,699] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 15:02:58,700] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 15:02:58,700] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 15:02:58,700] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 15:02:58,700] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 15:02:58,737] [INFO] [launch.py:256:main] process 50663 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 15:02:58,757] [INFO] [launch.py:256:main] process 50664 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 15:02:58,794] [INFO] [launch.py:256:main] process 50666 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 15:02:58,816] [INFO] [launch.py:256:main] process 50667 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-04 15:11:50,359] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 50663
[2025-05-04 15:11:50,378] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 50664
[2025-05-04 15:11:50,386] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 50666
[2025-05-04 15:11:50,386] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 50667
[2025-05-04 15:11:50,394] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561504: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:00:45 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:00:47 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 15:00:47 2025
Terminated at Sun May  4 15:11:52 2025
Results reported at Sun May  4 15:11:52 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1037.41 sec.
    Max Memory :                                 6336 MB
    Average Memory :                             3274.68 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   2 MB
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   665 sec.
    Turnaround time :                            667 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 15:19:26,997] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:20:20,888] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 15:20:20,888] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 15:20:38,982] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:21:24,116] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 15:21:24,116] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 15:21:24,116] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 15:21:24,116] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 15:21:24,116] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 15:21:24,143] [INFO] [launch.py:256:main] process 55443 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 15:21:24,159] [INFO] [launch.py:256:main] process 55444 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 15:21:24,187] [INFO] [launch.py:256:main] process 55445 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 15:21:24,214] [INFO] [launch.py:256:main] process 55446 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-04 15:30:33,789] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55443
[2025-05-04 15:30:33,864] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55444
[2025-05-04 15:30:33,873] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55445
[2025-05-04 15:30:33,874] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 55446
[2025-05-04 15:30:33,882] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561511: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:19:08 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:19:10 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 15:19:10 2025
Terminated at Sun May  4 15:30:35 2025
Results reported at Sun May  4 15:30:35 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1558.35 sec.
    Max Memory :                                 65366 MB
    Average Memory :                             27931.48 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                342
    Run time :                                   685 sec.
    Turnaround time :                            687 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 15:35:08,238] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:35:53,346] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 15:35:53,347] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 15:36:12,722] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:36:51,418] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 15:36:51,419] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 15:36:51,419] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 15:36:51,419] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 15:36:51,419] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 15:36:51,446] [INFO] [launch.py:256:main] process 59495 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 15:36:51,468] [INFO] [launch.py:256:main] process 59496 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 15:36:51,491] [INFO] [launch.py:256:main] process 59497 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 15:36:51,505] [INFO] [launch.py:256:main] process 59498 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 15:46:15,332] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:46:15,332] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:46:15,442] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:46:15,537] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:46:28,152] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 15:46:28,152] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 15:46:28,152] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 15:46:28,153] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-04 15:46:28,153] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 15:46:34,146] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59495
[2025-05-04 15:46:34,156] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59496
[2025-05-04 15:46:34,165] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59497
[2025-05-04 15:46:34,165] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 59498
[2025-05-04 15:46:34,172] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561528: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:34:42 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:34:43 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 15:34:43 2025
Terminated at Sun May  4 15:46:35 2025
Results reported at Sun May  4 15:46:35 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1677.75 sec.
    Max Memory :                                 128485 MB
    Average Memory :                             36744.23 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                346
    Run time :                                   716 sec.
    Turnaround time :                            713 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 15:58:09,510] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 15:59:02,844] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 15:59:02,845] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 15:59:29,011] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:00:24,922] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 16:00:24,922] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 16:00:24,922] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 16:00:24,922] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 16:00:24,922] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 16:00:24,948] [INFO] [launch.py:256:main] process 66070 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 16:00:24,965] [INFO] [launch.py:256:main] process 66071 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 16:00:24,980] [INFO] [launch.py:256:main] process 66072 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 16:00:25,000] [INFO] [launch.py:256:main] process 66073 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424

trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 16:08:17,549] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:08:17,549] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:08:17,549] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:08:17,549] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:08:30,779] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:08:30,779] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:08:30,779] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:08:30,779] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:08:30,794] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_160832-8cfeaad2[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_160832-e9edffbb[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_160832-f294a393[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_160832-1b3166dc[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.280128002166748 seconds
Time to load cpu_adam op: 3.306884527206421 seconds
Time to load cpu_adam op: 3.3084487915039062 seconds
Time to load cpu_adam op: 3.3125877380371094 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
 [1m[34mswanlab[0m[0m:ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
 ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 16:09:18,629] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66070
[2025-05-04 16:09:18,629] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66071
[2025-05-04 16:09:18,851] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66072
[2025-05-04 16:09:18,860] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 66073
[2025-05-04 16:09:18,868] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561547: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:57:46 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 15:57:48 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 15:57:48 2025
Terminated at Sun May  4 16:09:20 2025
Results reported at Sun May  4 16:09:20 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1822.10 sec.
    Max Memory :                                 144620 MB
    Average Memory :                             40197.40 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              201
    Max Threads :                                618
    Run time :                                   692 sec.
    Turnaround time :                            694 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 16:32:23,685] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:32:58,273] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 16:32:58,274] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 16:33:13,643] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:33:57,399] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 16:33:57,400] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 16:33:57,400] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 16:33:57,400] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 16:33:57,400] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 16:33:57,431] [INFO] [launch.py:256:main] process 75475 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 16:33:57,448] [INFO] [launch.py:256:main] process 75476 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 16:33:57,469] [INFO] [launch.py:256:main] process 75477 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 16:33:57,493] [INFO] [launch.py:256:main] process 75478 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 16:43:57,212] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:43:57,508] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:43:57,593] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:43:57,759] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 16:44:10,427] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:44:10,427] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:44:10,427] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-04 16:44:10,427] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 16:44:10,427] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_164412-83e20817[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_164412-b746801d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_164412-1cba8833[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_164412-0eda33d6[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.2297306060791016 secondsTime to load cpu_adam op: 3.229698657989502 secondsTime to load cpu_adam op: 3.229762077331543 seconds


Time to load cpu_adam op: 3.229933261871338 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:   Error happened while trainingError happened while training
Error happened while training

[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally 
ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m:  ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locallyğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally

[2025-05-04 16:44:57,203] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75475
[2025-05-04 16:44:57,425] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75476
[2025-05-04 16:44:57,435] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75477
[2025-05-04 16:44:57,444] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 75478
[2025-05-04 16:44:57,444] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561571: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 16:32:08 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 16:32:09 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 16:32:09 2025
Terminated at Sun May  4 16:44:59 2025
Results reported at Sun May  4 16:44:59 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2113.23 sec.
    Max Memory :                                 133763 MB
    Average Memory :                             29385.32 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              205
    Max Threads :                                622
    Run time :                                   769 sec.
    Turnaround time :                            771 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 17:03:05,739] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:03:43,685] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 17:03:43,685] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 17:03:57,106] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:04:37,630] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 17:04:37,630] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 17:04:37,630] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 17:04:37,630] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 17:04:37,630] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 17:04:37,655] [INFO] [launch.py:256:main] process 84193 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 17:04:37,671] [INFO] [launch.py:256:main] process 84194 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 17:04:37,684] [INFO] [launch.py:256:main] process 84195 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 17:04:37,696] [INFO] [launch.py:256:main] process 84196 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 17:14:22,513] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:14:22,513] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:14:22,513] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:14:22,644] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:14:33,666] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:14:33,666] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:14:33,666] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:14:33,667] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:14:33,680] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_171435-bf5d4be4[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_171435-7759e9db[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_171435-4bb92f72[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_171435-d397e44e[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.182197093963623 seconds
Time to load cpu_adam op: 3.2282583713531494 seconds
Time to load cpu_adam op: 3.2290470600128174 seconds
Time to load cpu_adam op: 3.231813430786133 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m: [1m[34mswanlab[0m[0m:Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training 
Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 17:15:23,391] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 84193
[2025-05-04 17:15:23,401] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 84194
[2025-05-04 17:15:23,410] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 84195
[2025-05-04 17:15:23,410] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 84196
[2025-05-04 17:15:23,417] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561606: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 17:02:42 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 17:02:44 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 17:02:44 2025
Terminated at Sun May  4 17:15:25 2025
Results reported at Sun May  4 17:15:25 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2099.58 sec.
    Max Memory :                                 133676 MB
    Average Memory :                             31475.89 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              201
    Max Threads :                                618
    Run time :                                   760 sec.
    Turnaround time :                            763 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 17:24:43,193] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:25:30,043] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 17:25:30,044] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 17:25:47,986] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:26:21,856] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 17:26:21,856] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 17:26:21,856] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 17:26:21,856] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 17:26:21,856] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 17:26:21,880] [INFO] [launch.py:256:main] process 90657 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 17:26:21,897] [INFO] [launch.py:256:main] process 90658 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 17:26:21,921] [INFO] [launch.py:256:main] process 90659 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 17:26:21,947] [INFO] [launch.py:256:main] process 90660 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 17:35:33,738] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:35:33,738] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:35:33,738] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:35:33,932] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 17:35:46,436] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:35:46,436] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:35:46,436] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 17:35:46,436] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-04 17:35:46,437] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_173548-8f84495b[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_173548-bcf3bc36[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_173548-503283c3[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_173548-76b6b014[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.1777021884918213 secondsTime to load cpu_adam op: 3.1777234077453613 secondsTime to load cpu_adam op: 3.1777398586273193 seconds


Time to load cpu_adam op: 3.2462100982666016 seconds
Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training
 Error happened while training
 Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 17:36:35,669] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90657
[2025-05-04 17:36:35,679] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90658
[2025-05-04 17:36:35,688] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90659
[2025-05-04 17:36:35,688] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 90660
[2025-05-04 17:36:35,695] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561632: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 17:24:26 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 17:24:28 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 17:24:28 2025
Terminated at Sun May  4 17:36:37 2025
Results reported at Sun May  4 17:36:37 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2088.79 sec.
    Max Memory :                                 133663 MB
    Average Memory :                             32695.11 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              201
    Max Threads :                                618
    Run time :                                   729 sec.
    Turnaround time :                            731 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 18:44:13,395] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 18:44:52,371] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=1,2,3,0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 18:44:52,371] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 18:45:07,145] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 18:45:47,018] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 18:45:47,018] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 18:45:47,018] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 18:45:47,018] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 18:45:47,018] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 18:45:47,043] [INFO] [launch.py:256:main] process 111316 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 18:45:47,060] [INFO] [launch.py:256:main] process 111317 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 18:45:47,074] [INFO] [launch.py:256:main] process 111318 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 18:45:47,095] [INFO] [launch.py:256:main] process 111319 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
[2025-05-04 18:47:02,173] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111316
[2025-05-04 18:47:02,173] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111317
[2025-05-04 18:47:02,183] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111318
[2025-05-04 18:47:02,192] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 111319
[2025-05-04 18:47:02,200] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561741: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 17:46:15 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 18:43:51 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 18:43:51 2025
Terminated at Sun May  4 18:47:04 2025
Results reported at Sun May  4 18:47:04 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   38.70 sec.
    Max Memory :                                 4132 MB
    Average Memory :                             2553.67 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              73
    Max Threads :                                178
    Run time :                                   193 sec.
    Turnaround time :                            3649 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 19:03:31,755] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:04:19,055] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=1,2,3,0 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 19:04:19,055] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 19:04:35,006] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:05:08,225] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 19:05:08,253] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 19:05:08,253] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 19:05:08,253] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 19:05:08,253] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 19:05:08,279] [INFO] [launch.py:256:main] process 2092 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 19:05:08,295] [INFO] [launch.py:256:main] process 2093 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 19:05:08,309] [INFO] [launch.py:256:main] process 2094 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 19:05:08,329] [INFO] [launch.py:256:main] process 2095 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 19:14:36,796] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:14:36,796] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:14:36,796] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:14:36,796] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:14:48,758] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 19:14:48,758] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-05-04 19:14:48,758] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 19:14:48,758] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 19:14:48,758] [INFO] [comm.py:669:init_distributed] cdb=None
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_191451-3214f29c[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_191451-956f2e6a[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_191451-aa3eb164[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_191451-7eae00c3[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
ninja: no work to do.
Time to load cpu_adam op: 3.5387566089630127 secondsTime to load cpu_adam op: 3.5387659072875977 seconds

Time to load cpu_adam op: 3.539151668548584 secondsTime to load cpu_adam op: 3.5391128063201904 seconds

Parameter Offload: Total persistent parameters: 3674112 in 193 params
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: Error happened while training 
Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m:[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
 ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 19:15:43,989] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2092
[2025-05-04 19:15:43,990] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2093
[2025-05-04 19:15:44,000] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2094
[2025-05-04 19:15:44,009] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 2095
[2025-05-04 19:15:44,016] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14561807: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 19:00:22 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 19:03:08 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 19:03:08 2025
Terminated at Sun May  4 19:15:45 2025
Results reported at Sun May  4 19:15:45 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   2116.07 sec.
    Max Memory :                                 144201 MB
    Average Memory :                             30633.96 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              201
    Max Threads :                                618
    Run time :                                   757 sec.
    Turnaround time :                            923 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 19:51:10,401] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:52:10,401] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 19:52:10,401] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=12345 --enable_each_rank_log=None training_swanlab.py
[2025-05-04 19:52:25,302] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 19:53:06,147] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 19:53:06,148] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 19:53:06,148] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 19:53:06,148] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 19:53:06,148] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 19:53:06,177] [INFO] [launch.py:256:main] process 15308 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0']
[2025-05-04 19:53:06,202] [INFO] [launch.py:256:main] process 15309 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1']
[2025-05-04 19:53:06,225] [INFO] [launch.py:256:main] process 15310 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2']
[2025-05-04 19:53:06,252] [INFO] [launch.py:256:main] process 15311 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3']
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424
[2025-05-04 20:02:29,988] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:02:29,988] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:02:29,988] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:02:29,988] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:02:47,258] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 20:02:47,258] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 20:02:47,259] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 20:02:47,259] [INFO] [comm.py:669:init_distributed] cdb=None
[2025-05-04 20:02:47,271] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_200249-43d5259d[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_200249-1b461766[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_200249-16afae30[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Tracking run with swanlab version 0.5.7
[1m[34mswanlab[0m[0m: Run data will be saved locally in [35m[1m/share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs/run-20250504_200249-ab300505[0m[0m
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[1m[34mswanlab[0m[0m: Error happened while training
[1m[34mswanlab[0m[0m: ğŸŒŸ Run `[1mswanlab watch /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/swan_logs[0m` to view SwanLab Experiment Dashboard locally
[2025-05-04 20:03:17,897] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 15308
[2025-05-04 20:03:17,907] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 15309
[2025-05-04 20:03:17,907] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 15310
[2025-05-04 20:03:17,916] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 15311
[2025-05-04 20:03:17,924] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14562052: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 19:50:48 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 19:50:51 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 19:50:51 2025
Terminated at Sun May  4 20:03:19 2025
Results reported at Sun May  4 20:03:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
~/pytorch/bin/deepspeed --num_gpus=4 --master_port=12345 training_swanlab.py

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   1300.02 sec.
    Max Memory :                                 11370 MB
    Average Memory :                             5364.07 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              205
    Max Threads :                                635
    Run time :                                   748 sec.
    Turnaround time :                            751 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

[2025-05-04 20:42:47,742] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:43:19,144] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected VISIBLE_DEVICES=0,1,2,3 but ignoring it because one or several of --include/--exclude/--num_gpus/--num_nodes cl args were used. If you want to use CUDA_VISIBLE_DEVICES don't pass any of these arguments to deepspeed.
[2025-05-04 20:43:19,145] [INFO] [runner.py:605:main] cmd = /share/home/zhangshanqi/pytorch/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None training_swanlab.py --deepspeed ds_config.json --per_device_train_batch_size 2 --gradient_accumulation_steps 8
[2025-05-04 20:43:34,759] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-05-04 20:44:04,037] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}
[2025-05-04 20:44:04,037] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=4, node_rank=0
[2025-05-04 20:44:04,037] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})
[2025-05-04 20:44:04,045] [INFO] [launch.py:164:main] dist_world_size=4
[2025-05-04 20:44:04,045] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
[2025-05-04 20:44:04,071] [INFO] [launch.py:256:main] process 28737 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=0', '--deepspeed', 'ds_config.json', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8']
[2025-05-04 20:44:04,088] [INFO] [launch.py:256:main] process 28738 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=1', '--deepspeed', 'ds_config.json', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8']
[2025-05-04 20:44:04,102] [INFO] [launch.py:256:main] process 28739 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=2', '--deepspeed', 'ds_config.json', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8']
[2025-05-04 20:44:04,115] [INFO] [launch.py:256:main] process 28740 spawned with command: ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3', '--deepspeed', 'ds_config.json', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8']
[2025-05-04 20:44:05,116] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28737
[2025-05-04 20:44:05,126] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28738
[2025-05-04 20:44:05,127] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28739
[2025-05-04 20:44:05,135] [INFO] [launch.py:319:sigkill_handler] Killing subprocess 28740
[2025-05-04 20:44:05,143] [ERROR] [launch.py:325:sigkill_handler] ['/share/home/zhangshanqi/pytorch/bin/python', '-u', 'training_swanlab.py', '--local_rank=3', '--deepspeed', 'ds_config.json', '--per_device_train_batch_size', '2', '--gradient_accumulation_steps', '8'] exits with return code = 1

------------------------------------------------------------
Sender: LSF System <lsfadmin@gpu08>
Subject: Job 14562100: <deepseek_train> in cluster <njucluster> Exited

Job <deepseek_train> was submitted from host <c01n04> by user <zhangshanqi> in cluster <njucluster> at Sun May  4 20:42:30 2025
Job was executed on host(s) <20*gpu08>, in queue <gpu_v100>, as user <zhangshanqi> in cluster <njucluster> at Sun May  4 20:42:32 2025
</share/home/zhangshanqi> was used as the home directory.
</share/home/zhangshanqi> was used as the working directory.
Started at Sun May  4 20:42:32 2025
Terminated at Sun May  4 20:44:07 2025
Results reported at Sun May  4 20:44:07 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
export TOKENIZERS_PARALLELISM=false
#swanlab ç¯å¢ƒæ¨¡å—
export SWANLAB_HOST=https://swanlab.cn
export SWANLAB_API_KEY=JtEPVvIMaPOLhVbHXivVl


#BSUB -q gpu_v100             # æŒ‡å®šä½¿ç”¨ gpu_v100 é˜Ÿåˆ—
#BSUB -J deepseek_train            # å®šä¹‰ä½œä¸šåç§°ï¼ˆå¯è‡ªå®šä¹‰ï¼‰
#BSUB -gpu "num=4:mode=shared"            # å®šä¹‰ä½¿ç”¨çš„ GPU æ•°é‡ï¼›å¦‚éœ€è¦å¤šå—GPUå¯ä¿®æ”¹ï¼Œå¦‚ "num=4"
#BSUB -n 20

#BSUB -o /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.out        # æ ‡å‡†è¾“å‡ºæ–‡ä»¶ï¼Œ%Jä¼šæ›¿æ¢ä¸ºä½œä¸šå·
#BSUB -e /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err              # é”™è¯¯è¾“å‡ºæ–‡ä»¶


# è®¾ç½®MPIå¯¹æ¥ç¯å¢ƒï¼ˆå¦‚ä½œä¸šä¸éœ€è¦MPIï¼Œå¯æ³¨é‡Šæ‰æˆ–ä¿ç•™ä»¥é˜²åç»­æ‰©å±•ï¼‰
export I_MPI_HYDRA_BOOTSTRAP=lsf

# è½½å…¥å¿…è¦çš„ç¯å¢ƒæ¨¡å—ï¼ˆæ ¹æ®ç³»ç»Ÿæƒ…å†µè°ƒæ•´ï¼‰
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/bin/compilervars.sh intel64
source /share/apps/intel/intel_2017u4/compilers_and_libraries_2017.4.196/linux/mkl/bin/mklvars.sh intel64

# è®¾ç½®CUDAç›¸å…³ç¯å¢ƒå˜é‡ï¼ˆç¡®ä¿èƒ½ä½¿ç”¨GPUåŠ é€Ÿåº“ï¼Œå¦‚TensorFlowã€PyTorchç­‰ï¼‰
export PATH=/share/gpu-apps/cuda/12.2/bin:$PATH
export LD_LIBRARY_PATH=/share/gpu-apps/cuda/12.2/lib64:$LD_LIBRARY_PATH
export PATH=~/pytorch/bin:$PATH
export LD_LIBRARY_PATH=/share/apps/gcc/gcc10.2/lib64:$LD_LIBRARY_PATH
# è¿›å…¥ä½œä¸šæ–‡ä»¶æ‰€åœ¨ç›®å½•
cd /share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained




 #æ‰§è¡Œ Python è„šæœ¬ï¼Œç¡®ä¿ train.py ä¸­æ­£ç¡®è°ƒç”¨äº†GPUåŠ é€Ÿç›¸å…³çš„
deepspeed --num_gpus=4 training_swanlab.py \
  --deepspeed ds_config.json \
  --per_device_train_batch_size 2 \
  --gradient_accumulation_steps 8

 
 


------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   13.55 sec.
    Max Memory :                                 1577 MB
    Average Memory :                             550.20 MB
    Total Requested Memory :                     -
    Delta Memory :                               -
    Max Swap :                                   -
    Max Processes :                              37
    Max Threads :                                73
    Run time :                                   94 sec.
    Turnaround time :                            97 sec.

The output (if any) is above this job summary.



PS:

Read file </share/home/zhangshanqi/pyn/shopping_behavior2/models/pretrained/output/train.err> for stderr output of this job.

